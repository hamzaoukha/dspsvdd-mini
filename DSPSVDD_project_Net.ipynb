{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f059ju7nc2_"
      },
      "source": [
        "# DSPSVDD — Deep Structure-Preserving SVDD for One-Class Anomaly Detection\n",
        "**Author:** Hamza Oukhacha & OUDRAIA Abdelouahad\n",
        "\n",
        "This notebook trains a DSPSVDD model for anomaly detection:\n",
        "1) Pretrain an AutoEncoder on normal samples\n",
        "2) Compute the hypersphere center in latent space\n",
        "3) Joint training (SVDD distance + reconstruction)\n",
        "4) Evaluation (ROC-AUC, PR-AUC, FPR@95%TPR, F1, MCC) and figures\n",
        "\n",
        "**Datasets:** MNIST / Fashion-MNIST  \n",
        "**Normal Class:** digit k (0..9)  \n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. Environment & Runtime Check  \n",
        "2. Mount Drive\n",
        "3. Configuration (CFG) & Utilities  \n",
        "4. Dataset & Dataloaders  \n",
        "5. Model: Convolutional AutoEncoder  \n",
        "6. Training: AE Warm-up  \n",
        "7. Hypersphere Center  \n",
        "8. Training: Joint DSPSVDD  \n",
        "9. Evaluation & Metrics  \n",
        "10. Plotting: ROC/PR/CM/Curves/Recons  \n",
        "11. Extra Figures (placeholders)  \n",
        "12. Save Artifacts  \n",
        "13. Single-Run Driver (save to Drive)  \n",
        "14. Multi-Run Driver (MNIST 1→9)  \n",
        "15. Aggregation to Summary CSV  \n",
        "16. Re-load a Run & Re-make Figures  \n",
        "17. Notes & Next Steps\n"
      ],
      "id": "9f059ju7nc2_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpDx4NbDnc3F"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Environment & Runtime Check\n",
        "import torch, torchvision, sys\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Torchvision:\", torchvision.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "id": "JpDx4NbDnc3F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBlr1Al8nc3H"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Mount Drive (optional but recommended)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "id": "qBlr1Al8nc3H"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9QKm9Vinc3I"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Configuration (CFG) & Utilities\n",
        "import os, time, json, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Config:\n",
        "    # Data\n",
        "    DATASET      = \"mnist\"       # \"mnist\" or \"fashion_mnist\"\n",
        "    NORMAL_CLASS = 1             # 0..9 you can changer to test or using all mnist (0 to 9)\n",
        "    IMG_SIZE     = 32\n",
        "    BATCH_SIZE   = 128\n",
        "    NUM_WORKERS  = 2\n",
        "\n",
        "    # Model\n",
        "    LATENT_DIM   = 128\n",
        "\n",
        "    # Training\n",
        "    AE_EPOCHS    = 15\n",
        "    JOINT_EPOCHS = 20\n",
        "    LR           = 1e-3\n",
        "    LR_JOINT     = 5e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    GAMMA        = 0.1\n",
        "    NU           = 0.05\n",
        "\n",
        "    # System\n",
        "    DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    SEED         = 42\n",
        "    OUTPUT_DIR   = \"./dspsvdd_results\"\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "def cfg_to_dict():\n",
        "    return {k: getattr(CFG, k) for k in dir(CFG) if k.isupper() and not k.startswith(\"_\")}\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ],
      "id": "a9QKm9Vinc3I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3AkWewSnc3J"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Dataset & Dataloaders\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class OneClassDataset(Dataset):\n",
        "    \"\"\"Binary labels for test: 0=normal (digit==NORMAL_CLASS), 1=anomaly (otherwise).\"\"\"\n",
        "    def __init__(self, dataset, normal_class: int):\n",
        "        self.dataset = dataset\n",
        "        self.nc = int(normal_class)\n",
        "    def __len__(self): return len(self.dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[idx]\n",
        "        return x, 0 if int(y) == self.nc else 1\n",
        "\n",
        "def _get_targets(dataset):\n",
        "    if hasattr(dataset, \"targets\"):\n",
        "        t = dataset.targets\n",
        "        return t.cpu().numpy() if isinstance(t, torch.Tensor) else np.array(t)\n",
        "    ys = []\n",
        "    for _, y in dataset: ys.append(int(y))\n",
        "    return np.array(ys)\n",
        "\n",
        "def get_data_loaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((CFG.IMG_SIZE, CFG.IMG_SIZE)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    if CFG.DATASET.lower() == \"mnist\":\n",
        "        train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "        test_dataset  = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "    elif CFG.DATASET.lower() == \"fashion_mnist\":\n",
        "        train_dataset = datasets.FashionMNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "        test_dataset  = datasets.FashionMNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "    else:\n",
        "        raise ValueError(\"DATASET must be 'mnist' or 'fashion_mnist'.\")\n",
        "\n",
        "    targets = _get_targets(train_dataset)\n",
        "    idx_normals = np.where(targets == int(CFG.NORMAL_CLASS))[0].tolist()\n",
        "    train_subset = Subset(train_dataset, idx_normals)\n",
        "\n",
        "    test_bin = OneClassDataset(test_dataset, CFG.NORMAL_CLASS)\n",
        "    pin = torch.cuda.is_available()\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=CFG.BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=CFG.NUM_WORKERS, pin_memory=pin)\n",
        "    test_loader  = DataLoader(test_bin,    batch_size=CFG.BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=CFG.NUM_WORKERS, pin_memory=pin)\n",
        "    return train_loader, test_loader\n"
      ],
      "id": "q3AkWewSnc3J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SY8Ho31nc3K"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Model: Convolutional AutoEncoder\n",
        "class ConvolutionalAutoEncoder(nn.Module):\n",
        "    \"\"\"Small, stable conv AE for 1x32x32 images.\"\"\"\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1), nn.ReLU(True),   # 32x16x16\n",
        "            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(True),  # 64x8x8\n",
        "            nn.Conv2d(64, 128,4, 2, 1), nn.ReLU(True),  # 128x4x4\n",
        "        )\n",
        "        self.encoder_fc = nn.Linear(128*4*4, latent_dim)\n",
        "        self.decoder_fc = nn.Linear(latent_dim, 128*4*4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 1, 4, 2, 1),   nn.Sigmoid()\n",
        "        )\n",
        "        self.apply(self._init)\n",
        "\n",
        "    def _init(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x).view(x.size(0), -1)\n",
        "        return self.encoder_fc(h)\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder_fc(z).view(z.size(0), 128, 4, 4)\n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        xhat = self.decode(z)\n",
        "        return z, xhat\n"
      ],
      "id": "4SY8Ho31nc3K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZA8vcGnc3M"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Training: AE Warm-up\n",
        "def train_autoencoder(model, loader, epochs, lr, weight_decay, device):\n",
        "    \"\"\"Warm-up AE to learn structure before SVDD joint training.\"\"\"\n",
        "    print(\"Training AutoEncoder...\")\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    mse = nn.MSELoss()\n",
        "    losses = []\n",
        "    for ep in range(1, epochs+1):\n",
        "        s, n = 0.0, 0\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            _, xhat = model(x)\n",
        "            loss = mse(xhat, x)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            s += loss.item(); n += 1\n",
        "        losses.append(s / max(n, 1))\n",
        "        print(f\"Epoch [{ep}/{epochs}] AE Loss: {losses[-1]:.6f}\")\n",
        "    return losses\n"
      ],
      "id": "5cZA8vcGnc3M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1yPtKnSnc3M"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Hypersphere Center\n",
        "@torch.no_grad()\n",
        "def compute_center(model, loader, device):\n",
        "    \"\"\"Mean latent vector over normal samples; small epsilon to avoid collapse.\"\"\"\n",
        "    print(\"Computing hypersphere center...\")\n",
        "    model.eval()\n",
        "    Z = []\n",
        "    for x, _ in loader:\n",
        "        x = x.to(device)\n",
        "        z, _ = model(x)\n",
        "        Z.append(z.detach().cpu())\n",
        "    c = torch.cat(Z, dim=0).mean(dim=0).to(device)\n",
        "    eps = 1e-6\n",
        "    c = torch.where(c.abs() < eps, eps * torch.sign(c + eps), c)\n",
        "    return c\n"
      ],
      "id": "Z1yPtKnSnc3M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS9oXpZHnc3N"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Training: Joint DSPSVDD\n",
        "def train_dspsvdd(model, loader, center, epochs, gamma, lr, weight_decay, device):\n",
        "    \"\"\"Joint objective: ||z - c||^2 + gamma * MSE(x, x_hat).\"\"\"\n",
        "    print(\"Training DSPSVDD...\")\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    mse = nn.MSELoss()\n",
        "    losses, dist_losses, recon_losses = [], [], []\n",
        "    for ep in range(1, epochs+1):\n",
        "        s_tot = s_d = s_r = 0.0; n = 0\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            z, xhat = model(x)\n",
        "            dloss = ((z - center) ** 2).sum(dim=1).mean()\n",
        "            rloss = mse(xhat, x)\n",
        "            loss = dloss + gamma * rloss\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            s_tot += loss.item(); s_d += dloss.item(); s_r += rloss.item(); n += 1\n",
        "        losses.append(s_tot / n); dist_losses.append(s_d / n); recon_losses.append(s_r / n)\n",
        "        print(f\"Epoch [{ep}/{epochs}] Total: {losses[-1]:.6f} | Dist: {dist_losses[-1]:.6f} | Recon: {recon_losses[-1]:.6f}\")\n",
        "    return losses, dist_losses, recon_losses\n"
      ],
      "id": "WS9oXpZHnc3N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwPBh45nc3O"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title valuation & Metrics\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, roc_curve,\n",
        "                             precision_recall_curve, confusion_matrix, f1_score, matthews_corrcoef)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, loader, center, device):\n",
        "    \"\"\"Return per-sample: distance^2, recon error, and binary label.\"\"\"\n",
        "    print(\"Evaluating model...\")\n",
        "    model.eval()\n",
        "    D2, MSEs, Y = [], [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        z, xhat = model(x)\n",
        "        d2 = ((z - center) ** 2).sum(dim=1).cpu().numpy()\n",
        "        mse = F.mse_loss(xhat, x, reduction=\"none\").mean(dim=[1,2,3]).cpu().numpy()\n",
        "        D2.append(d2); MSEs.append(mse); Y.append(y.numpy())\n",
        "    return np.concatenate(D2), np.concatenate(MSEs), np.concatenate(Y)\n",
        "\n",
        "def compute_metrics(labels, scores, threshold=None):\n",
        "    \"\"\"AUCs, FPR@95%TPR; +Accuracy/F1/MCC if threshold is provided.\"\"\"\n",
        "    m = {}\n",
        "    m[\"roc_auc\"] = float(roc_auc_score(labels, scores))\n",
        "    m[\"pr_auc\"]  = float(average_precision_score(labels, scores))\n",
        "    fpr, tpr, _ = roc_curve(labels, scores)\n",
        "    idx = np.where(tpr >= 0.95)[0]\n",
        "    m[\"fpr_at_95_tpr\"] = float(fpr[idx[0]]) if len(idx) > 0 else 1.0\n",
        "    if threshold is not None:\n",
        "        pred = (scores > threshold).astype(int)\n",
        "        m[\"accuracy\"] = float((pred == labels).mean())\n",
        "        m[\"f1_score\"] = float(f1_score(labels, pred, zero_division=0))\n",
        "        m[\"mcc\"]      = float(matthews_corrcoef(labels, pred))\n",
        "        tn, fp, fn, tp = confusion_matrix(labels, pred).ravel()\n",
        "        m[\"tn\"], m[\"fp\"], m[\"fn\"], m[\"tp\"] = int(tn), int(fp), int(fn), int(tp)\n",
        "    return m\n"
      ],
      "id": "kFwPBh45nc3O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyMLgJyHnc3P"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Plotting: ROC/PR/CM/Curves/Recons\n",
        "def _savefig(path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    plt.savefig(path, dpi=300, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "def plot_roc_pr(distances, recon_errors, labels, outdir):\n",
        "    # ROC\n",
        "    plt.figure(figsize=(6,5))\n",
        "    fpr_d, tpr_d, _ = roc_curve(labels, distances)\n",
        "    fpr_r, tpr_r, _ = roc_curve(labels, recon_errors)\n",
        "    auc_d, auc_r = roc_auc_score(labels, distances), roc_auc_score(labels, recon_errors)\n",
        "    plt.plot(fpr_d, tpr_d, label=f\"Distance (AUC={auc_d:.3f})\")\n",
        "    plt.plot(fpr_r, tpr_r, label=f\"Recon (AUC={auc_r:.3f})\")\n",
        "    plt.plot([0,1],[0,1],\"k--\", alpha=0.4)\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curves\"); plt.legend(); plt.grid(alpha=0.25)\n",
        "    _savefig(os.path.join(outdir, \"roc.png\"))\n",
        "    # PR\n",
        "    plt.figure(figsize=(6,5))\n",
        "    pre_d, rec_d, _ = precision_recall_curve(labels, distances)\n",
        "    pre_r, rec_r, _ = precision_recall_curve(labels, recon_errors)\n",
        "    ap_d, ap_r = average_precision_score(labels, distances), average_precision_score(labels, recon_errors)\n",
        "    plt.plot(rec_d, pre_d, label=f\"Distance (AP={ap_d:.3f})\")\n",
        "    plt.plot(rec_r, pre_r, label=f\"Recon (AP={ap_r:.3f})\")\n",
        "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR Curves\"); plt.legend(); plt.grid(alpha=0.25)\n",
        "    _savefig(os.path.join(outdir, \"pr.png\"))\n",
        "\n",
        "def plot_training_curves(ae_losses, joint_losses, dist_losses, recon_losses, outdir):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(range(1, len(ae_losses)+1), ae_losses, label=\"AE Loss\")\n",
        "    xs = range(len(ae_losses)+1, len(ae_losses)+len(joint_losses)+1)\n",
        "    plt.plot(xs, joint_losses, label=\"DSPSVDD Total\")\n",
        "    plt.plot(xs, dist_losses,  label=\"Distance Loss\")\n",
        "    plt.plot(xs, recon_losses, label=\"Recon Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Curves\")\n",
        "    plt.legend(); plt.grid(alpha=0.25)\n",
        "    _savefig(os.path.join(outdir, \"training_curves.png\"))\n",
        "\n",
        "def plot_cm(labels, scores, thr, outdir):\n",
        "    pred = (scores > thr).astype(int)\n",
        "    cm = confusion_matrix(labels, pred)\n",
        "    plt.figure(figsize=(4.5,4))\n",
        "    plt.imshow(cm, cmap=\"Blues\"); plt.title(\"Confusion Matrix @ threshold\"); plt.colorbar()\n",
        "    for (i,j),v in np.ndenumerate(cm): plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    plt.xticks([0,1], [\"Normal\",\"Anomaly\"]); plt.yticks([0,1], [\"Normal\",\"Anomaly\"])\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    _savefig(os.path.join(outdir, \"cm.png\"))\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_sample_reconstructions(model, loader, device, outdir, num=8):\n",
        "    model.eval()\n",
        "    x, y = next(iter(loader))\n",
        "    x = x[:num].to(device); y = y[:num]\n",
        "    _, xhat = model(x)\n",
        "    plt.figure(figsize=(2*num, 4))\n",
        "    for i in range(num):\n",
        "        ax = plt.subplot(2, num, i+1)\n",
        "        ax.imshow(x[i,0].cpu().numpy(), cmap=\"gray\"); ax.axis(\"off\")\n",
        "        ax.set_title(\"Normal\" if int(y[i])==0 else \"Anomaly\")\n",
        "        ax = plt.subplot(2, num, num+i+1)\n",
        "        ax.imshow(xhat[i,0].cpu().numpy(), cmap=\"gray\"); ax.axis(\"off\")\n",
        "        ax.set_title(\"Recon\")\n",
        "    plt.suptitle(\"Original (top) vs Reconstruction (bottom)\")\n",
        "    _savefig(os.path.join(outdir, \"samples_recon.png\"))\n"
      ],
      "id": "uyMLgJyHnc3P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtdWSg6Jnc3Q"
      },
      "source": [
        "## Extra Figures\n",
        "For Visualizations(histograms, PR/F1 vs threshold, t-SNE, error grids)._\n"
      ],
      "id": "HtdWSg6Jnc3Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b0eEFPXnc3R"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Save Artifacts\n",
        "def save_artifacts(CFG, ae_losses, joint_losses, dist_losses, recon_losses,\n",
        "                   t_train, test_d2, test_mse, y_test, train_d2, train_mse, thr_d2, thr_mse,\n",
        "                   m_dist, m_recon, model, center):\n",
        "    results = {\n",
        "        \"config\": cfg_to_dict(),\n",
        "        \"training\": {\n",
        "            \"time_seconds\": float(t_train),\n",
        "            \"final_ae_loss\": float(ae_losses[-1]),\n",
        "            \"final_joint_loss\": float(joint_losses[-1]),\n",
        "            \"final_distance_loss\": float(dist_losses[-1]),\n",
        "            \"final_recon_loss\": float(recon_losses[-1]),\n",
        "        },\n",
        "        \"evaluation\": {\n",
        "            \"distance_metrics\": m_dist,\n",
        "            \"reconstruction_metrics\": m_recon,\n",
        "            \"thresholds\": {\"distance\": thr_d2, \"reconstruction\": thr_mse}\n",
        "        }\n",
        "    }\n",
        "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "    with open(os.path.join(CFG.OUTPUT_DIR, \"results.json\"), \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    hist_df = pd.DataFrame({\n",
        "        \"epoch\": list(range(1, CFG.AE_EPOCHS + CFG.JOINT_EPOCHS + 1)),\n",
        "        \"ae_loss\":        ae_losses + [np.nan]*CFG.JOINT_EPOCHS,\n",
        "        \"joint_total\":    [np.nan]*CFG.AE_EPOCHS + joint_losses,\n",
        "        \"joint_distance\": [np.nan]*CFG.AE_EPOCHS + dist_losses,\n",
        "        \"joint_recon\":    [np.nan]*CFG.AE_EPOCHS + recon_losses,\n",
        "    })\n",
        "    hist_df.to_csv(os.path.join(CFG.OUTPUT_DIR, \"training_history.csv\"), index=False)\n",
        "\n",
        "    ckpt = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"center\": center.detach().cpu(),\n",
        "        \"threshold_distance\": thr_d2,\n",
        "        \"threshold_reconstruction\": thr_mse,\n",
        "        \"config\": cfg_to_dict()\n",
        "    }\n",
        "    torch.save(ckpt, os.path.join(CFG.OUTPUT_DIR, \"model.pth\"))\n"
      ],
      "id": "-b0eEFPXnc3R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIiMiDFrnc3R"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Single-Run Driver (save to Drive)\n",
        "def set_drive_output(normal_cls, dataset=\"mnist\", base_dir=\"/content/drive/MyDrive/Master/DSPSVDD\"):\n",
        "    \"\"\"Create a clean output folder: .../<dataset>/nc_<k>_<timestamp>/\"\"\"\n",
        "    CFG.DATASET = dataset\n",
        "    CFG.NORMAL_CLASS = int(normal_cls)\n",
        "    stamp = time.strftime(\"%Y%m%d_%H%M\")\n",
        "    CFG.OUTPUT_DIR = os.path.join(base_dir, dataset, f\"nc_{CFG.NORMAL_CLASS}_{stamp}\")\n",
        "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "    print(\"[Output]\", CFG.OUTPUT_DIR)\n",
        "\n",
        "def main():\n",
        "    print(\"DSPSVDD Anomaly Detection\")\n",
        "    print(f\"Dataset: {CFG.DATASET} | Normal class: {CFG.NORMAL_CLASS} | Device: {CFG.DEVICE}\")\n",
        "    print(\"-\"*55)\n",
        "    set_seed(CFG.SEED)\n",
        "    os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    train_loader, test_loader = get_data_loaders()\n",
        "    print(f\"Training samples: {len(train_loader.dataset)} | Test samples: {len(test_loader.dataset)}\")\n",
        "    model = ConvolutionalAutoEncoder(CFG.LATENT_DIM).to(CFG.DEVICE)\n",
        "\n",
        "    t0 = time.time()\n",
        "    ae_losses = train_autoencoder(model, train_loader, CFG.AE_EPOCHS, CFG.LR, CFG.WEIGHT_DECAY, CFG.DEVICE)\n",
        "    center = compute_center(model, train_loader, CFG.DEVICE)\n",
        "    joint_losses, dist_losses, recon_losses = train_dspsvdd(\n",
        "        model, train_loader, center, CFG.JOINT_EPOCHS, CFG.GAMMA, CFG.LR_JOINT, CFG.WEIGHT_DECAY, CFG.DEVICE\n",
        "    )\n",
        "    t_train = time.time() - t0\n",
        "\n",
        "    test_d2,  test_mse,  y_test  = evaluate_model(model, test_loader, center, CFG.DEVICE)\n",
        "    train_d2, train_mse, _y_tr   = evaluate_model(model, train_loader, center, CFG.DEVICE)\n",
        "\n",
        "    thr_d2  = float(np.quantile(train_d2,  1.0 - CFG.NU))\n",
        "    thr_mse = float(np.quantile(train_mse, 1.0 - CFG.NU))\n",
        "\n",
        "    m_dist  = compute_metrics(y_test, test_d2,  thr_d2)\n",
        "    m_recon = compute_metrics(y_test, test_mse, thr_mse)\n",
        "\n",
        "    # Figures\n",
        "    plot_roc_pr(test_d2, test_mse, y_test, CFG.OUTPUT_DIR)\n",
        "    plot_training_curves(ae_losses, joint_losses, dist_losses, recon_losses, CFG.OUTPUT_DIR)\n",
        "    plot_cm(y_test, test_d2, thr_d2, CFG.OUTPUT_DIR)\n",
        "    save_sample_reconstructions(model, test_loader, CFG.DEVICE, CFG.OUTPUT_DIR, num=8)\n",
        "\n",
        "    # Save artifacts\n",
        "    save_artifacts(CFG, ae_losses, joint_losses, dist_losses, recon_losses,\n",
        "                   t_train, test_d2, test_mse, y_test, train_d2, train_mse, thr_d2, thr_mse,\n",
        "                   m_dist, m_recon, model, center)\n",
        "\n",
        "    print(\"\\nSaved results to:\", CFG.OUTPUT_DIR)\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "def run_one(normal_cls, dataset=\"mnist\", base_dir=\"/content/drive/MyDrive/Master/DSPSVDD\"):\n",
        "    set_drive_output(normal_cls, dataset, base_dir)\n",
        "    main()\n",
        "    return CFG.OUTPUT_DIR\n"
      ],
      "id": "JIiMiDFrnc3R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXqpjhTWnc3S"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Multi-Run Driver (MNIST 1→9)\n",
        "def run_all_mnist_1_to_9(base_dir=\"/content/drive/MyDrive/Master/DSPSVDD\"): # u can change this path independe for ur situation\n",
        "    folders = []\n",
        "    for k in range(1, 10):\n",
        "        print(f\"\\n=== Running normal class = {k} ===\")\n",
        "        folders.append(run_one(k, dataset=\"mnist\", base_dir=base_dir))\n",
        "    print(\"\\nFinished. Folders:\")\n",
        "    for d in folders: print(\" -\", d)\n",
        "    return folders\n",
        "\n"
      ],
      "id": "OXqpjhTWnc3S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaKyf6ZOnc3T"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Aggregation to Summary CSV\n",
        "import glob\n",
        "\n",
        "def aggregate_runs(dataset='mnist', base_dir=\"/content/drive/MyDrive/Master/DSPSVDD\"): # mnist or fushion_mnist\n",
        "    root = os.path.join(base_dir, dataset)\n",
        "    pattern_flat   = glob.glob(os.path.join(root, \"nc_*_*\"))\n",
        "    pattern_nested = glob.glob(os.path.join(root, \"nc_*\", \"*\"))\n",
        "    run_dirs = sorted([p for p in (pattern_flat + pattern_nested) if os.path.isdir(p)])\n",
        "    if not run_dirs:\n",
        "        print(\"No runs found under:\", root)\n",
        "        return None\n",
        "\n",
        "    rows = []\n",
        "    for rd in run_dirs:\n",
        "        rjson = os.path.join(rd, \"results.json\")\n",
        "        if not os.path.exists(rjson):\n",
        "            continue\n",
        "        parts = os.path.basename(rd).split('_')\n",
        "        nc = None\n",
        "        if len(parts) >= 2 and parts[0] == \"nc\":\n",
        "            try: nc = int(parts[1])\n",
        "            except: pass\n",
        "        if nc is None:\n",
        "            parent = os.path.basename(os.path.dirname(rd))\n",
        "            if parent.startswith(\"nc_\"):\n",
        "                try: nc = int(parent.split('_')[1])\n",
        "                except: pass\n",
        "        with open(rjson, \"r\") as f:\n",
        "            R = json.load(f)\n",
        "        dist = R[\"evaluation\"][\"distance_metrics\"]\n",
        "        rec  = R[\"evaluation\"][\"reconstruction_metrics\"]\n",
        "        thr  = R[\"evaluation\"][\"thresholds\"]\n",
        "        trn  = R[\"training\"]\n",
        "        cfg  = R[\"config\"]\n",
        "\n",
        "        rows.append({\n",
        "            \"dataset\": dataset, \"normal_class\": nc,\n",
        "            \"ae_epochs\": cfg.get(\"AE_EPOCHS\"), \"joint_epochs\": cfg.get(\"JOINT_EPOCHS\"),\n",
        "            \"roc_auc_distance\": dist.get(\"roc_auc\"), \"pr_auc_distance\": dist.get(\"pr_auc\"),\n",
        "            \"fpr_at_95_distance\": dist.get(\"fpr_at_95_tpr\"),\n",
        "            \"accuracy_distance\": dist.get(\"accuracy\"), \"f1_distance\": dist.get(\"f1_score\"),\n",
        "            \"mcc_distance\": dist.get(\"mcc\"),\n",
        "            \"roc_auc_recon\": rec.get(\"roc_auc\"), \"pr_auc_recon\": rec.get(\"pr_auc\"),\n",
        "            \"thr_distance\": thr.get(\"distance\"), \"thr_reconstruction\": thr.get(\"reconstruction\"),\n",
        "            \"train_time_s\": trn.get(\"time_seconds\"),\n",
        "            \"final_ae_loss\": trn.get(\"final_ae_loss\"),\n",
        "            \"final_joint_loss\": trn.get(\"final_joint_loss\"),\n",
        "            \"output_dir\": rd\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"normal_class\", \"output_dir\"])\n",
        "    stamp = time.strftime(\"%Y%m%d_%H%M\")\n",
        "    out_csv = os.path.join(root, f\"summary_{dataset}_{stamp}.csv\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(\"Summary saved:\", out_csv)\n",
        "    try:\n",
        "        from caas_jupyter_tools import display_dataframe_to_user\n",
        "        display_dataframe_to_user(f\"DSPSVDD summary ({dataset})\", df)\n",
        "    except Exception:\n",
        "        print(df.head())\n",
        "    return out_csv\n"
      ],
      "id": "WaKyf6ZOnc3T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnToUVk2nc3T"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# @title Re-load a Run & Re-make Figures (This step is optinal)\n",
        "def load_latest_run_for_class(normal_cls, base_dir=\"/content/drive/MyDrive/Master/DSPSVDD/mnist\"):\n",
        "    \"\"\"Reload newest nc_k_* folder, rebuild loaders, reload checkpoint, recompute vectors.\"\"\"\n",
        "    import glob\n",
        "    CFG.NORMAL_CLASS = int(normal_cls)\n",
        "    matches = sorted(glob.glob(os.path.join(base_dir, f\"nc_{CFG.NORMAL_CLASS}_*\")), key=os.path.getmtime)\n",
        "    assert matches, f\"No runs found for class {CFG.NORMAL_CLASS}\"\n",
        "    CFG.OUTPUT_DIR = matches[-1]\n",
        "\n",
        "    # rebuild loaders\n",
        "    global train_loader, test_loader\n",
        "    train_loader, test_loader = get_data_loaders()\n",
        "\n",
        "    # reload ckpt\n",
        "    ckpt = torch.load(os.path.join(CFG.OUTPUT_DIR, \"model.pth\"), map_location=CFG.DEVICE)\n",
        "    global model, center, test_d2, test_mse, y_test, train_d2, train_mse, thr_d2, thr_mse\n",
        "    model = ConvolutionalAutoEncoder(CFG.LATENT_DIM).to(CFG.DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"]); model.eval()\n",
        "    center = ckpt[\"center\"].to(CFG.DEVICE)\n",
        "\n",
        "    # recompute\n",
        "    test_d2,  test_mse,  y_test  = evaluate_model(model, test_loader, center, CFG.DEVICE)\n",
        "    train_d2, train_mse, _y_tr   = evaluate_model(model, train_loader, center, CFG.DEVICE)\n",
        "    thr_d2  = float(np.quantile(train_d2,  1.0 - CFG.NU))\n",
        "    thr_mse = float(np.quantile(train_mse, 1.0 - CFG.NU))\n",
        "    print(f\"[OK] Reloaded class {CFG.NORMAL_CLASS} from:\", CFG.OUTPUT_DIR)"
      ],
      "id": "tnToUVk2nc3T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTeW6RlBnc3U"
      },
      "source": [
        "## Notes & Next Steps\n",
        "- Keep `NUM_WORKERS` modest (2–4) on Colab to avoid dataloader hangs.  \n",
        "- Always rebuild loaders + reload checkpoint when switching classes to avoid stale globals.  \n",
        "- For faster debugging, lower AE/Joint epochs; restore defaults for final results.\n",
        "\n",
        "**Next Steps**\n",
        "- Add fused scoring (distance + scaled reconstruction).  \n",
        "- Evaluate across all digits (1→9) and Fashion-MNIST; report mean/median AUC.  \n",
        "- Add per-anomaly-digit ROC analysis for MNIST to discuss difficulty differences.\n"
      ],
      "id": "xTeW6RlBnc3U"
    }
  ]
}
